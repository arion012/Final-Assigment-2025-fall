---
title: 'Dungeons and Dragons: monsters'
author: "Sebesty√©n V.Nagy"
output: html_document
---
```{r, include=FALSE}
options(repos = c(CRAN = "https://cran.rstudio.com/"))
install.packages("tidyr")
install.packages("Rsampling")
library(Rsampling)
install.packages("recipes")
library(recipes)
install.packages("parsnip")
library(parsnip)
install.packages("tune")
library(tune)
install.packages("yardstick")
library(yardstick)
library(tidyr)
library(tidyverse)
library(dplyr)
library(psych)
install.packages("tidymodels")
library(tidymodels)
library(car)
library(lmtest)

```
# Reading the data in
```{r, include=TRUE}
DandData <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/refs/heads/main/data/2025/2025-05-27/monsters.csv")
view(DandData)
describe(DandData)
```
#Making some factorization
```{r, include=TRUE}

DandData<- DandData %>% 
  mutate(size = factor(size),
         type = factor(type),
         category = factor(category),
         alignment =factor(alignment))

summary(DandData)
```

# Visualizing some trends

```{r, include=TRUE}
DandData %>% 
  select(ac, cr) %>% 
  ggplot() +
  aes(x = ac, y = cr) +
  geom_point() +
  geom_smooth()

DandData %>%
ggplot() +
aes(x = size, fill = type) +
geom_bar()

```

# Exploring the data with suprevised learning models

```{r, include=TRUE}
Ddsplit <- initial_split(DandData,
                         prop = 0.75,
                         strata = cr)

lm_model <- linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression")

Ddlastfit <- lm_model %>% 
  last_fit(cr ~ ac,
           split = Ddsplit)

Ddlastfit %>% 
  collect_metrics()
ddlastfit_model1 <- Ddlastfit %>% 
  collect_predictions()


ggplot(ddlastfit_model1, aes(x = cr, y = .pred)) +
  geom_point() +
  geom_smooth()


```
Well, its seems that the rsquare is 0.678 which is pretty high but I think it could be improved still, with some other variable. And the Rmse is pretty high 3.81 considering Cr goes up to 30 so the model is amiss with more than 10% on average. 


# Building a complex supervised learning model
This time predicting that monsters with higher intelligence, wisdom and charisma would score higher in challange rating because they can subdue the adventurers via manipulation
```{r, include=TRUE}

supmodel2 <- lm_model %>% 
  last_fit(cr ~ ac + int + cha + wis,
           split = Ddsplit)

supmodel2 %>% 
  collect_metrics()
supmodel2_pred <- supmodel2 %>% 
  collect_predictions()


ggplot(supmodel2_pred, aes(x = cr, y = .pred)) +
  geom_point() +
  geom_smooth()

```

The hypothesis doesnt seem to stand the model slightly is worse than the simple one, with 0.629 rsq it explains smaller percentage of the data, and with rmse 3.94 it misses the mark by a higher amount.

Lets give another to a more complex model but this time lets hypothesize that stronger, sturdier (more hp) and more dexterous enemies would score a higher cr

```{r, include=TRUE}

supmodel3 <- lm_model %>% 
  last_fit(cr ~ ac + str + dex + hp_number,
           split = Ddsplit)

supmodel3 %>% 
  collect_metrics()
supmodel3_pred <- supmodel3 %>% 
  collect_predictions()


ggplot(supmodel3_pred, aes(x = cr, y = .pred)) +
  geom_point() +
  geom_smooth()

```
Oh okay, it seems that this is way more accurate than our previous two hypothesises, with a 0.947 rsq its outstanding at explaining the data coming much closer to fully explainig it fully, rmse is way smaller 1.69. It seems that creatures with higher cr are usually way more stronger, more agile and usually with more HP. But now lets check it with more in depth linear regression model to understand which predictors play a key factor in explaining cr 

# Building a simple linear regression model
```{r, include=TRUE}
reg_model1 <- lm(cr ~ ac + str + dex + hp_number, DandData)
summary(reg_model1)
```
Well, its seems that the rsquare is 0.678 which is pretty high but I think it could be improved still, with some other variable. And the Rmse is pretty high 3.81 considering Cr goes up to 30 so the model is amiss with more than 10% on average. 

# Testing the model

```{r, include=TRUE}
AIC(reg_model1) #pretty high AIC which means that the model doesn't not fit that well, and there is a high value of errors also but in itself we cant use this information just now, but when we build the second model. 

reg_model1 %>% 
  plot(which = 4)

reg_model1 %>% 
  plot(which = 5)

#From checking high leverage cases and cook distance we found that there are three data points which are highly influential on the model. Altough this can modify our results lets check the following results of the data to see if the requirements for regression are still fulfilled. 

reg_model1 %>% 
  plot(which = 2)
describe(residuals(reg_model1))
#skewness is alright with 0.59 but the kurtosis isn't the best its well over 5.2, it can mean that the normality requirement is unmet, could be because of the influential cases.

reg_model1 %>% 
  residualPlots()
#linearity is definitely unmet with hp_number this time. The other ones are pretty solid with linearity. It seems possible that including the Hp can hurt the model. But lets check further... 

reg_model1 %>% 
  plot(which = 3)
reg_model1 %>% 
  bptest()
#homoscedasticity is also significant in the model. Which means that the residuals are not spread out similarly in the model.

reg_model1 %>% 
  vif()
#but at least it seems that multicorrealinity isn't problematic. 
```

So the model isn't that great, there are some influential cases which distort our results, we might want to exclude them to have a better fit for the overall data
```{r, include=TRUE}
Data_modified <- DandData[-c(95, 171, 204), ]
summary(Data_modified)
describe(Data_modified)
view(Data_modified)

reg_model2 <- lm(cr ~ ac + str + dex + hp_number, Data_modified)
summary(reg_model2)

reg_model2 %>% 
  plot(which = 4)
describe(residuals(reg_model2))
reg_model2 %>% 
  residualPlots()
reg_model2 %>% 
  plot(which = 3)
reg_model2 %>% 
  bptest()
reg_model2 %>% 
  vif()
AIC(reg_model2)
```
# Results from the second regression

The second model is better, kurtosis is less (Still quite high = 3,81), but on AIC the model rates better, RSE is also lower and adjustedrsquare a bit better. We had to exclude three outliers to achieve this (Tarrasque, Ancient Gold Dragon and Ancient Red dragon) because their hp distored the model. Altough dex and str doesnt show a strong correlation with the cr at all, so we might just well exclude them from the predictors and try out another predictor.

```{r, include=TRUE}
res <- aov(hp_number ~ size, data = Data_modified) #checking whether there is a difference between HP based on the size category, there might be an interaction going on between these two predictors. 
summary(res) #its seems like its a certain possibility

reg_model3 <- lm(cr ~ ac + hp_number * size, Data_modified)
summary(reg_model3)

reg_model3 %>% 
  residualPlots()
reg_model3 %>% 
  bptest()
reg_model3 %>% 
  plot(which = 3)
reg_model3 %>% 
  plot(which = 4)
describe(residuals(reg_model3))
reg_model3 %>% 
  vif()

Data_modified1 <- Data_modified[-c(171, 129, 145), ]
reg_model4 <- lm(cr ~ ac + hp_number * size, Data_modified1)
summary(reg_model4)

reg_model4 %>% 
  residualPlots()
reg_model4 %>% 
  bptest()
reg_model4 %>% 
  plot(which = 3)
reg_model4 %>% 
  plot(which = 4)
describe(residuals(reg_model4))
reg_model4 %>% 
  vif()

AIC(reg_model4)
```
# Results from the third regression

Well, we found that size is a much better predictor, combined with AC it explains 0.948 of the residuals. In the mean time it seem to interact with Hp_number so we got a pretty much working modell now. We could still improve on this by excluding some outliers based on the cooks distance (4/n) (ancient red dragon, lemure and mummy) and it also significantly improves the AIC. The interaction weak point is only with small and tiny creatures, it comes short with them. We also impacted the curtosis by the trimmings it got higher compared to the previous model. 


```{r, include=TRUE}
finalsplit <- initial_split(Data_modified1,
                         prop = 0.75,
                         strata = cr)

supmodelfinal <- lm_model %>% 
  last_fit(cr ~ ac + hp_number * size, 
           split = finalsplit)

supmodelfinal %>% 
  collect_metrics()
supmodel2_pred <- supmodel2 %>% 
  collect_predictions()
supmodel2_pred
```
# Verdict

We managed to assert a supervised algorithm which does its job fairly well on guessing cr of monsters, which are not some outlying cases. But with an 1.23 rmse the mistakes are quite low and rsq is quite high (9.50) explaining quite large part of the residuals.
I hope this algorithm will be somewhat useful for the futures adventurers for guessing cr of monsters. 